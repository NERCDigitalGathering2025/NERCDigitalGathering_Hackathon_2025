{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåç NERC Digital Gathering - Environmental Data Hackathon ‚Äî Soils & Land Cover Notebook\n",
        "\n",
        "Welcome to the NERC Digital Gathering hackathon!  \n",
        "This notebook contains the challenge briefs and starter code for you to explore weather, climate, and soil data available through CEDA and other sources. In the hackathon we invite you to use NERC data and to explore the CEDA archive. However, Cranfield hold the national soil map and so if your hack involves soil you can also use that dataset too.\n",
        "\n",
        "In the hackathon, we are offering a chance to explore and interact with a range of meteorological ands other data in CEDA - NERC's Centre for Environmental Data Analysis. The datasets we are looking at include ECMWF, HAD and MIDAS. These data are in different formats and structures so you can explore these differences as well.\n",
        "\n",
        "**MIDAS** (Met Office Integrated Data Archive System): This is a database of raw weather observations from land and marine surface stations, both in the UK and globally. It contains daily, hourly and sub-hourly measurements of various parameters like temperature, rainfall, sunshine, wind, cloud cover, and present weather codes. MIDAS data is station based timeseries data in CSV format.\n",
        "\n",
        "**ECMWF** (European Centre for Medium-Range Weather Forecasts): This organisation produces weather forecasts and climate reanalyses. ECMWF data includes estimates of atmospheric parameters like air temperature, pressure, and wind at different altitudes, as well as surface parameters like rainfall, soil moisture content, ocean-wave height, and sea-surface temperature, for the entire globe. They also have ocean reanalysis and analysis systems like OCEAN5. ECMWF has regional gridded data in NetCDF format.\n",
        "\n",
        "**HadUK-Grid** is a dataset of gridded climate variables for the UK derived from interpolated land surface observations. It focuses on climate variables like temperature, rainfall, sunshine, mean sea level pressure, wind speed, relative humidity, vapour pressure, days of snow lying, and days of ground frost, at daily, monthly, seasonal, and annual timescales. HADUK has regional gridded data in NetCDF format.\n",
        "\n",
        "**Soils and Land Cover** In addition to the meteorological notebooks, we are also running a fourth notebook that allows some comparison of soil types and land cover in the county of Bedfordshire. Our challenge is to undertake some spatial analysis to establish any patterns between the datasets. \n",
        "\n",
        "**This notebook sets some challenges using soils and land cover data for Bedfordshire.**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Getting Started\n",
        "\n",
        "1. **Load libraries**  \n",
        "   The sorts of libraries you may need include `pandas`, `numpy`, `matplotlib`, `seaborn`, and `geopandas`.  \n",
        "   (Install with `pip install ...` if missing.)\n",
        "\n",
        "2. **Accessing external data**  \n",
        "   Example:\n",
        "   ```python\n",
        "   import pandas as pd\n",
        "   url = \"https://nercdigitalgathering2025.github.io/data/Bedfordshire_CORINE_Landcover_2018_100mGrid.csv\"\n",
        "   df = pd.read_csv(url)\n",
        "   print(df.head())\n",
        "   ```\n",
        "\n",
        "3. **Notebook structure**  \n",
        "   Each challenge is introduced in Markdown with background, tasks, and judging criteria.  \n",
        "   Under each challenge you'll find starter code cells to help you begin.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Geographical focus\n",
        "In this hackathon, we will focus the hacking geographically. You can choose to look at the UK as a whole, or focus in on Bedfordshire where we are located. Geographical coordinates for these areas are as follows:\n",
        "\n",
        "* UK bounding box (roughly -10¬∞W to 3¬∞E, 49¬∞N to 61¬∞N)\n",
        "* Bedfordshire bounding box (roughly -0.89¬∞W to 0.23¬∞E, 51.95¬∞N to 52.49¬∞N)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Useful links\n",
        "Here are a few useful web addressses for CEDA data:\n",
        "* CEDA Data home: https://data.ceda.ac.uk\n",
        "* CEDA Help Doc home: https://help.ceda.ac.uk\n",
        "* MIDAS User Guide: https://zenodo.org/records/7357335\n",
        "* ECMWF website - https://www.ecmwf.int\n",
        "* JASMIN Notebooks service help: https://help.jasmin.ac.uk/docs/interactive-computing/jasmin-notebooks-service/\n",
        "\n",
        "---\n",
        "\n",
        "# üìù Challenges\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 1 ‚Äî Load and Explore Land Cover Data\n",
        "\n",
        "### Background\n",
        "CORINE (Coordination of Information on the Environment) Land Cover data provides detailed information about land use and land cover across Europe. The Bedfordshire dataset contains 100m grid resolution data showing different land cover types such as agricultural areas, forests, urban areas, and water bodies.\n",
        "\n",
        "### Your Task\n",
        "Load the CORINE Land Cover dataset and perform initial exploration:\n",
        "- Load the CSV file from the provided URL\n",
        "- Examine the data structure, columns, and data types\n",
        "- Generate basic statistics about land cover distribution\n",
        "- Create visualisations showing land cover patterns\n",
        "\n",
        "**Success criteria:** successful data loading, clear understanding of data structure, informative statistics and visualisations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for data processing, analysis, and visualisation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from urllib.request import urlretrieve\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style for better visualisations\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure pandas display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define URLs for the datasets\n",
        "landcover_url = \"https://nercdigitalgathering2025.github.io/data/Bedfordshire_CORINE_Landcover_2018_100mGrid.csv\"\n",
        "soils_url = \"https://nercdigitalgathering2025.github.io/data/Bedfordshire_NATMAP_Soils_100mGrid.csv\"\n",
        "\n",
        "# Load the CORINE Land Cover dataset\n",
        "print(\"Loading CORINE Land Cover data...\")\n",
        "landcover_df = pd.read_csv(landcover_url)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(f\"Dataset shape: {landcover_df.shape}\")\n",
        "print(f\"Columns: {list(landcover_df.columns)}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "landcover_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine data types and basic statistics\n",
        "print(\"Data types:\")\n",
        "print(landcover_df.dtypes)\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(landcover_df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(landcover_df.isnull().sum())\n",
        "\n",
        "# Display unique values in categorical columns (if any)\n",
        "print(\"\\nUnique values in each column:\")\n",
        "for col in landcover_df.columns:\n",
        "    unique_count = landcover_df[col].nunique()\n",
        "    print(f\"{col}: {unique_count} unique values\")\n",
        "    if unique_count <= 20:  # Show values if not too many\n",
        "        # Handle mixed data types by converting to string and removing NaN values\n",
        "        unique_values = landcover_df[col].dropna().astype(str).unique()\n",
        "        try:\n",
        "            # Try to sort, but handle cases where sorting might fail\n",
        "            sorted_values = sorted(unique_values)\n",
        "            print(f\"  Values: {sorted_values}\")\n",
        "        except TypeError:\n",
        "            # If sorting fails, just show the unique values without sorting\n",
        "            print(f\"  Values: {list(unique_values)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 2 ‚Äî Load and Explore Soils Data\n",
        "\n",
        "### Background\n",
        "The NATMAP Soils dataset provides detailed soil information for Bedfordshire at 100m grid resolution. This dataset contains various soil properties including soil type, texture, drainage characteristics, and other physical and chemical properties that are crucial for understanding agricultural potential, flood risk, and environmental processes.\n",
        "\n",
        "### Your Task\n",
        "Load the NATMAP Soils dataset and perform comprehensive analysis:\n",
        "- Load the CSV file from the provided URL\n",
        "- Examine the data structure and identify different soil properties\n",
        "- Generate statistics for numerical soil properties\n",
        "- Analyse categorical soil classifications\n",
        "- Create visualisations of soil distribution patterns\n",
        "\n",
        "**Success criteria:** successful data loading, thorough understanding of soil data structure, comprehensive statistical analysis, clear visualisations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the NATMAP Soils dataset\n",
        "print(\"Loading NATMAP Soils data...\")\n",
        "soils_df = pd.read_csv(soils_url)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(f\"Dataset shape: {soils_df.shape}\")\n",
        "print(f\"Columns: {list(soils_df.columns)}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "soils_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine data types and basic statistics\n",
        "print(\"Data types:\")\n",
        "print(soils_df.dtypes)\n",
        "print(\"\\nBasic statistics for numerical columns:\")\n",
        "print(soils_df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(soils_df.isnull().sum())\n",
        "\n",
        "# Identify numerical vs categorical columns\n",
        "numerical_cols = soils_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = soils_df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "\n",
        "# Display unique values in categorical columns\n",
        "print(\"\\nUnique values in categorical columns:\")\n",
        "for col in categorical_cols:\n",
        "    unique_count = soils_df[col].nunique()\n",
        "    print(f\"{col}: {unique_count} unique values\")\n",
        "    if unique_count <= 15:  # Show values if not too many\n",
        "        # Handle mixed data types by converting to string and removing NaN values\n",
        "        unique_values = soils_df[col].dropna().astype(str).unique()\n",
        "        try:\n",
        "            # Try to sort, but handle cases where sorting might fail\n",
        "            sorted_values = sorted(unique_values)\n",
        "            print(f\"  Values: {sorted_values}\")\n",
        "        except TypeError:\n",
        "            # If sorting fails, just show the unique values without sorting\n",
        "            print(f\"  Values: {list(unique_values)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 3 ‚Äî Statistical Analysis and Visualisation\n",
        "\n",
        "### Background\n",
        "Understanding the distribution and patterns in both land cover and soils data is essential for environmental analysis. Statistical analysis helps identify dominant land cover types, soil characteristics, and spatial patterns that can inform land management decisions.\n",
        "\n",
        "### Your Task\n",
        "Perform comprehensive statistical analysis and create informative visualisations:\n",
        "- Generate frequency distributions for land cover types\n",
        "- Analyse soil property distributions and correlations\n",
        "- Create spatial visualisations of both datasets\n",
        "- Identify patterns and anomalies in the data\n",
        "- Compare the spatial extent and coverage of both datasets\n",
        "\n",
        "**Success criteria:** comprehensive statistical analysis, clear and informative visualisations, identification of key patterns and relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyse land cover distribution\n",
        "print(\"=== LAND COVER ANALYSIS ===\")\n",
        "\n",
        "# Identify the main land cover column (assuming it's one of the columns)\n",
        "# This will need to be adjusted based on actual column names\n",
        "landcover_cols = [col for col in landcover_df.columns if col == 'Description']\n",
        "print(f\"Potential land cover columns: {landcover_cols}\")\n",
        "\n",
        "# If we can identify a land cover type column, analyse it\n",
        "if landcover_cols:\n",
        "    main_col = landcover_cols[0]\n",
        "    print(f\"\\nAnalysing column: {main_col}\")\n",
        "    \n",
        "    # Frequency analysis\n",
        "    landcover_counts = landcover_df[main_col].value_counts()\n",
        "    print(\"\\nLand cover type frequencies:\")\n",
        "    print(landcover_counts)\n",
        "    \n",
        "    # Create visualisation - Bar plot of land cover types\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    landcover_counts.plot(kind='bar')\n",
        "    plt.title('Land Cover Type Distribution', fontsize=16)\n",
        "    plt.xlabel('Land Cover Type', fontsize=14)\n",
        "    plt.ylabel('Frequency', fontsize=14)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Pie chart of land cover types\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    landcover_counts.plot(kind='pie', autopct='%1.1f%%')\n",
        "    plt.title('Land Cover Type Proportions', fontsize=16)\n",
        "    plt.ylabel('')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Could not identify land cover column. Please examine the data structure above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyse soils data - Focus on categorical/object data types\n",
        "print(\"=== SOILS ANALYSIS ===\")\n",
        "\n",
        "# Analyse categorical soil properties (object data types)\n",
        "if categorical_cols:\n",
        "    print(f\"\\nAnalysing {len(categorical_cols)} categorical soil properties:\")\n",
        "    \n",
        "    # Create visualisations for each categorical property\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        print(f\"\\n=== {col.upper()} ANALYSIS ===\")\n",
        "        \n",
        "        # Get value counts\n",
        "        value_counts = soils_df[col].value_counts()\n",
        "        print(f\"Total unique values: {len(value_counts)}\")\n",
        "        print(f\"Top 10 most common values:\")\n",
        "        print(value_counts.head(10))\n",
        "        \n",
        "        # Create visualisation for this categorical property\n",
        "        plt.figure(figsize=(16, 8))\n",
        "        \n",
        "        # Bar chart of value counts\n",
        "        value_counts.plot(kind='bar')\n",
        "        plt.title(f'Distribution of {col}', fontsize=16)\n",
        "        plt.xlabel(col, fontsize=14)\n",
        "        plt.ylabel('Frequency', fontsize=14)\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "        plt.yticks(fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Show percentage distribution\n",
        "        print(f\"\\nPercentage distribution (top 10):\")\n",
        "        percentages = (value_counts.head(10) / len(soils_df) * 100).round(2)\n",
        "        for value, pct in percentages.items():\n",
        "            print(f\"{value}: {pct}%\")\n",
        "        \n",
        "        # If there are too many categories, show summary\n",
        "        if len(value_counts) > 20:\n",
        "            print(f\"\\nNote: {col} has {len(value_counts)} unique values. Showing top 10 only.\")\n",
        "            print(f\"Other {len(value_counts) - 10} values represent {((value_counts.iloc[10:].sum() / len(soils_df)) * 100):.1f}% of data\")\n",
        "\n",
        "# Cross-tabulation analysis between categorical soil properties\n",
        "if len(categorical_cols) >= 2:\n",
        "    print(f\"\\n=== CROSS-TABULATION ANALYSIS ===\")\n",
        "    \n",
        "    # Create cross-tabulation between first two categorical columns\n",
        "    col1, col2 = categorical_cols[0], categorical_cols[1]\n",
        "    print(f\"Cross-tabulation between {col1} and {col2}:\")\n",
        "    \n",
        "    # Create crosstab\n",
        "    crosstab = pd.crosstab(soils_df[col1], soils_df[col2], margins=True)\n",
        "    print(crosstab)\n",
        "    \n",
        "    # Create heatmap of crosstab (excluding margins for cleaner visualisation)\n",
        "    crosstab_clean = pd.crosstab(soils_df[col1], soils_df[col2])\n",
        "    \n",
        "    # Only show heatmap if not too large\n",
        "    if crosstab_clean.shape[0] <= 20 and crosstab_clean.shape[1] <= 20:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(crosstab_clean, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f'Cross-tabulation: {col1} vs {col2}', fontsize=16)\n",
        "        plt.xlabel(col2, fontsize=14)\n",
        "        plt.ylabel(col1, fontsize=14)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Crosstab too large for heatmap ({crosstab_clean.shape[0]}x{crosstab_clean.shape[1]}). Showing data only.\")\n",
        "\n",
        "else:\n",
        "    print(\"Need at least 2 categorical columns for cross-tabulation analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 4 ‚Äî Spatial Analysis and Grid Comparison\n",
        "\n",
        "### Background\n",
        "Both datasets are provided at 100m grid resolution for Bedfordshire. Understanding the spatial relationship between land cover and soil properties is crucial for environmental analysis, agricultural planning, and ecosystem management.\n",
        "\n",
        "### Your Task\n",
        "Perform spatial analysis and intercomparison of the datasets:\n",
        "- Identify common grid coordinates between the datasets\n",
        "- Create spatial visualisations showing the distribution of both datasets\n",
        "- Analyse the relationship between land cover types and soil properties\n",
        "- Identify areas of overlap and potential data gaps\n",
        "- Create summary statistics for grid cell comparisons\n",
        "\n",
        "**Success criteria:** successful spatial analysis, clear understanding of dataset relationships, informative spatial visualisations, meaningful intercomparison results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spatial analysis and grid comparison\n",
        "print(\"=== SPATIAL ANALYSIS ===\")\n",
        "\n",
        "# Identify coordinate columns (assuming they contain 'x', 'y', 'lon', 'lat', 'east', 'north')\n",
        "coord_keywords = ['x', 'y', 'lon', 'lat', 'east', 'north', 'coord']\n",
        "landcover_coords = [col for col in landcover_df.columns if any(keyword in col.lower() for keyword in coord_keywords)]\n",
        "soils_coords = [col for col in soils_df.columns if any(keyword in col.lower() for keyword in coord_keywords)]\n",
        "\n",
        "print(f\"Land cover coordinate columns: {landcover_coords}\")\n",
        "print(f\"Soils coordinate columns: {soils_coords}\")\n",
        "\n",
        "# Check if we have coordinate information\n",
        "if landcover_coords and soils_coords:\n",
        "    print(\"\\nCoordinate information found in both datasets\")\n",
        "    \n",
        "    # Display coordinate ranges\n",
        "    print(\"\\nLand cover coordinate ranges:\")\n",
        "    for col in landcover_coords:\n",
        "        print(f\"{col}: {landcover_df[col].min():.2f} to {landcover_df[col].max():.2f}\")\n",
        "    \n",
        "    print(\"\\nSoils coordinate ranges:\")\n",
        "    for col in soils_coords:\n",
        "        print(f\"{col}: {soils_df[col].min():.2f} to {soils_df[col].max():.2f}\")\n",
        "    \n",
        "    # Create spatial scatter plots if we have 2D coordinates\n",
        "    if len(landcover_coords) >= 2 and len(soils_coords) >= 2:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Land cover spatial plot\n",
        "        axes[0].scatter(landcover_df[landcover_coords[0]], landcover_df[landcover_coords[1]], \n",
        "                       alpha=0.6, s=1, c='blue')\n",
        "        axes[0].set_xlabel(landcover_coords[0])\n",
        "        axes[0].set_ylabel(landcover_coords[1])\n",
        "        axes[0].set_title('Land Cover Data Spatial Distribution')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Soils spatial plot\n",
        "        axes[1].scatter(soils_df[soils_coords[0]], soils_df[soils_coords[1]], \n",
        "                       alpha=0.6, s=1, c='red')\n",
        "        axes[1].set_xlabel(soils_coords[0])\n",
        "        axes[1].set_ylabel(soils_coords[1])\n",
        "        axes[1].set_title('Soils Data Spatial Distribution')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Coordinate information not clearly identified. Please examine the data structure above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 5 ‚Äî Advanced Intercomparison Analysis\n",
        "\n",
        "### Background\n",
        "Understanding the relationships between land cover and soil properties is crucial for environmental management, agricultural planning, and ecosystem services assessment. Advanced analysis can reveal patterns that inform land use decisions and environmental policy.\n",
        "\n",
        "### Your Task\n",
        "Perform advanced intercomparison analysis between land cover and soils data:\n",
        "- Create cross-tabulation analysis between land cover types and soil properties\n",
        "- Identify correlations between land cover and soil characteristics\n",
        "- Perform statistical tests to identify significant relationships\n",
        "- Create comprehensive visualisations showing land cover-soil relationships\n",
        "- Generate summary reports of key findings\n",
        "\n",
        "**Success criteria:** thorough intercomparison analysis, identification of meaningful relationships, clear statistical interpretation, comprehensive visualisations, actionable insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced intercomparison analysis - Focus on categorical relationships\n",
        "print(\"=== ADVANCED INTERCOMPARISON ANALYSIS ===\")\n",
        "\n",
        "# Use specific coordinate fields for merging\n",
        "coord_cols = ['East_1K', 'North_1K']\n",
        "\n",
        "# Check if both datasets have the required coordinate columns\n",
        "if all(col in landcover_df.columns for col in coord_cols) and all(col in soils_df.columns for col in coord_cols):\n",
        "    print(\"Creating merged dataset using East_1K and North_1K coordinates...\")\n",
        "    \n",
        "    # Create coordinate-based merge\n",
        "    landcover_df['coord_key'] = list(zip(landcover_df['East_1K'], landcover_df['North_1K']))\n",
        "    soils_df['coord_key'] = list(zip(soils_df['East_1K'], soils_df['North_1K']))\n",
        "\n",
        "    #print(landcover_df.head())\n",
        "    #print(soils_df.head())\n",
        "    \n",
        "    # Merge datasets on coordinates\n",
        "    merged_df = pd.merge(landcover_df, soils_df, on='coord_key', how='inner', suffixes=('_lc', '_soil'))\n",
        "    #print(merged_df.head())\n",
        "    \n",
        "    # Display the first few rows of the merged dataset\n",
        "    print(\"\\nFirst few rows of merged dataset:\")\n",
        "    merged_df.head()\n",
        "    \n",
        "    print(f\"Merged dataset shape: {merged_df.shape}\")\n",
        "    print(f\"Successfully merged {len(merged_df)} grid cells\")\n",
        "    \n",
        "    if len(merged_df) > 0:\n",
        "        # Focus on categorical analysis between land cover Description and soil object fields\n",
        "        print(\"\\n=== CATEGORICAL INTERCOMPARISON ANALYSIS ===\")\n",
        "        \n",
        "        # Identify the Description column in land cover data\n",
        "        lc_desc_col = 'Description_lc' if 'Description_lc' in merged_df.columns else None\n",
        "        \n",
        "        # Identify categorical columns from soils data (excluding coordinate columns)\n",
        "        soil_categorical_cols = [col for col in merged_df.columns \n",
        "                               if col.endswith('_soil') and \n",
        "                               pd.api.types.is_object_dtype(merged_df[col]) and\n",
        "                               not any(coord in col for coord in ['East', 'North', 'coord'])]\n",
        "        \n",
        "        print(f\"Land cover description column: {lc_desc_col}\")\n",
        "        print(f\"Soil categorical columns: {soil_categorical_cols}\")\n",
        "        \n",
        "        if lc_desc_col and soil_categorical_cols:\n",
        "            # Create cross-tabulation analysis for each soil categorical field vs land cover description\n",
        "            for soil_col in soil_categorical_cols:\n",
        "                print(f\"\\n=== {soil_col.upper()} vs LAND COVER DESCRIPTION ===\")\n",
        "                \n",
        "                # Create cross-tabulation\n",
        "                crosstab = pd.crosstab(merged_df[soil_col], merged_df[lc_desc_col], margins=True)\n",
        "                print(f\"Cross-tabulation between {soil_col} and {lc_desc_col}:\")\n",
        "                print(crosstab)\n",
        "                \n",
        "                # Create visualisation if not too large\n",
        "                crosstab_clean = pd.crosstab(merged_df[soil_col], merged_df[lc_desc_col])\n",
        "                \n",
        "                if crosstab_clean.shape[0] <= 15 and crosstab_clean.shape[1] <= 15:\n",
        "                    plt.figure(figsize=(14, 10))\n",
        "                    sns.heatmap(crosstab_clean, annot=True, fmt='d', cmap='Blues', \n",
        "                               cbar_kws={'label': 'Count'})\n",
        "                    plt.title(f'Cross-tabulation: {soil_col} vs Land Cover Description', fontsize=16)\n",
        "                    plt.xlabel('Land Cover Description', fontsize=14)\n",
        "                    plt.ylabel(soil_col, fontsize=14)\n",
        "                    plt.xticks(rotation=45, ha='right')\n",
        "                    plt.yticks(rotation=0)\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "                else:\n",
        "                    print(f\"Crosstab too large for heatmap ({crosstab_clean.shape[0]}x{crosstab_clean.shape[1]}).\")\n",
        "                \n",
        "                # Calculate chi-square test for independence\n",
        "                from scipy.stats import chi2_contingency\n",
        "                chi2, p_value, dof, expected = chi2_contingency(crosstab_clean)\n",
        "                print(f\"\\nChi-square test for independence:\")\n",
        "                print(f\"Chi-square statistic: {chi2:.4f}\")\n",
        "                print(f\"p-value: {p_value:.6f}\")\n",
        "                print(f\"Degrees of freedom: {dof}\")\n",
        "                if p_value < 0.05:\n",
        "                    print(\"Result: Significant association between variables (p < 0.05)\")\n",
        "                else:\n",
        "                    print(\"Result: No significant association between variables (p >= 0.05)\")\n",
        "                \n",
        "                # Show most common combinations\n",
        "                print(f\"\\nTop 10 most common combinations:\")\n",
        "                combinations = merged_df.groupby([soil_col, lc_desc_col]).size().sort_values(ascending=False)\n",
        "                print(combinations.head(10))\n",
        "        \n",
        "        # Summary statistics\n",
        "        print(f\"\\n=== MERGED DATA SUMMARY ===\")\n",
        "        print(f\"Total merged grid cells: {len(merged_df):,}\")\n",
        "        print(f\"Coverage area: {len(merged_df) * 100 * 100 / 1_000_000:.2f} km¬≤\")\n",
        "        \n",
        "        # Data completeness\n",
        "        print(\"\\nData completeness in merged dataset:\")\n",
        "        completeness = (1 - merged_df.isnull().sum() / len(merged_df)) * 100\n",
        "        for col, comp in completeness.items():\n",
        "            if comp < 100:\n",
        "                print(f\"{col}: {comp:.1f}% complete\")\n",
        "    \n",
        "    else:\n",
        "        print(\"No overlapping data found between datasets.\")\n",
        "        \n",
        "else:\n",
        "    print(\"Cannot perform intercomparison analysis - missing East_1K or North_1K coordinate columns.\")\n",
        "    print(f\"Land cover columns: {list(landcover_df.columns)}\")\n",
        "    print(f\"Soils columns: {list(soils_df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Epilogue\n",
        "In this notebook, a comprehensive framework for analysing soils and land cover data has been provided. You have explored data loading, statistical analysis, spatial visualisation, and intercomparison techniques that can be applied to environmental datasets.\n",
        "\n",
        "**Extension ideas:**\n",
        "- Integrate with climate data from the other notebooks for comprehensive environmental analysis\n",
        "- Develop machine learning models to predict soil properties from land cover data\n",
        "- Create interactive dashboards for stakeholder engagement\n",
        "- Perform spatial clustering analysis to identify distinct environmental zones\n",
        "- Develop land suitability models for agricultural or conservation planning\n",
        "- Integrate with remote sensing data for temporal analysis\n",
        "- Create ecosystem services assessment frameworks\n",
        "- Develop flood risk models combining soils, land cover, and climate data\n",
        "\n",
        "This analysis framework provides a solid foundation for environmental data science applications and can be adapted for various research and planning purposes.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
